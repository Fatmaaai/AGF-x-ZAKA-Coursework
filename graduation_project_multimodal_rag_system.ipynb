{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvQjg1ZvZb498O+CZ664mu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fatmaaai/AGF-x-ZAKA-Coursework/blob/main/graduation_project_multimodal_rag_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is our Multimodal rag system - graduation project development collabration notebook. In this notebook, we will explore different components of the system and run it using streamlit.\n",
        "\n",
        "**Team Members:**\n",
        "\n",
        "* Fatma Egal\n",
        "* Mai Ahmed\n",
        "\n",
        "\n",
        "**Supervised under:**\n",
        "* Dr. Karim & Naim\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Project Goals:**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "1.   Develop a robust and efficient multimodel RAG system.\n",
        "2.   Experiment with different LLM models, embedding models, and retrieval strategies.\n",
        "3. Explore and implement advanced features\n",
        "4.   Design and implement a user-friendly Streamlit interface.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Project Structure:**\n",
        "\n",
        "This notebook will be organized into different sections:\n",
        "\n",
        "1. **Data Loading and Preprocessing:**\n",
        "    * Data loading and cleaning.\n",
        "    * Text splitting and chunking.\n",
        "2. **Embedding Generation:**\n",
        "    * Embedding model selection and experimentation.\n",
        "    * Embedding generation and storage.\n",
        "3. **Retrieval:**\n",
        "    * Index creation and management and retrieval.\n",
        "4. **Generation:**\n",
        "    * LLM model selection and prompting.\n",
        "    * Response generation and evaluation.\n",
        "5. **User Interface (Streamlit):**\n",
        "    * UI design and development.\n",
        "    * User interaction and feedback mechanisms.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**Collaboration Guidelines:**\n",
        "\n",
        "* Use comments extensively to explain your code and document your findings.\n",
        "* Make sure to save the changes you have done in the colab notebook.\n",
        "* Clearly communicate and discuss any challenges or roadblocks encountered in our meetings.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**Time to learn, learn & learn together!**"
      ],
      "metadata": {
        "id": "AO4mFcvZSQKW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MAIN**"
      ],
      "metadata": {
        "id": "DhnGMrQsRAkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "from multimodal_rag import MultimodalRAGWithModelSelection\n",
        "import tempfile\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Set Hugging Face token if available\n",
        "if os.getenv(\"HUGGINGFACE_TOKEN\"):\n",
        "    os.environ[\"HUGGINGFACE_TOKEN\"] = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
        "\n",
        "# Set page configuration\n",
        "st.set_page_config(page_title=\"Multimodal RAG System\", layout=\"wide\")\n",
        "\n",
        "# Title and description\n",
        "st.title(\"Multimodal RAG System\")\n",
        "st.write(\"Upload a PDF document and ask questions about its content.  For LM Studio models, ensure LM Studio is running with the selected model and the server is active.\")\n",
        "\n",
        "# Initialize session state\n",
        "if 'rag_system' not in st.session_state:\n",
        "    st.session_state.rag_system = MultimodalRAGWithModelSelection()\n",
        "\n",
        "if 'pdf_processed' not in st.session_state:\n",
        "    st.session_state.pdf_processed = False\n",
        "\n",
        "# Sidebar for model selection\n",
        "st.sidebar.title(\"Model Selection\")\n",
        "model_options = {\n",
        "    \"T5 Small (Hugging Face)\": \"google/flan-t5-small\",\n",
        "    \"Phi-3.5-mini-instruct (LM Studio)\": \"Phi-3.5-mini-instruct\",  #  Adjust names as needed\n",
        "    \"Falcon-7B (LM Studio)\": \"Falcon-7B\",\n",
        "    \"Llama 3.1 (LM Studio)\": \"Llama 3.1\"\n",
        "}\n",
        "\n",
        "selected_model = st.sidebar.selectbox(\n",
        "    \"Select LLM Model\",\n",
        "    list(model_options.keys()),\n",
        "    index=0\n",
        ")\n",
        "\n",
        "# Change model if selection changed\n",
        "if st.session_state.get('selected_model') != selected_model:\n",
        "    st.session_state.selected_model = selected_model\n",
        "    model_name = model_options[selected_model]\n",
        "    if 'rag_system' in st.session_state:\n",
        "        try:\n",
        "            st.session_state.rag_system.change_model(model_name)\n",
        "            st.success(f\"Model changed to {model_name}\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error changing model: {e}\")\n",
        "            st.stop()\n",
        "\n",
        "# File uploader - trying again\n",
        "uploaded_file = st.file_uploader(\"Upload your PDF\", type=[\"pdf\"])\n",
        "\n",
        "if uploaded_file:\n",
        "    # Save the uploaded file to a temporary location\n",
        "    with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:\n",
        "        tmp_file.write(uploaded_file.getvalue())\n",
        "        pdf_path = tmp_file.name\n",
        "\n",
        "    # Process the PDF\n",
        "    if not st.session_state.pdf_processed:\n",
        "        try:\n",
        "            stats = st.session_state.rag_system.process_pdf(pdf_path)\n",
        "            st.session_state.pdf_processed = True\n",
        "            st.success(\"PDF processed successfully\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error processing PDF: {e}\")\n",
        "            st.stop()\n",
        "\n",
        "\n",
        "# Query input\n",
        "if st.session_state.pdf_processed and st.session_state.rag_system.llm is not None:  # Check if LLM is initialized\n",
        "    st.subheader(\"Ask a Question\")\n",
        "    query = st.text_input(\"Enter your question about the document\")\n",
        "\n",
        "    if query:\n",
        "        try:\n",
        "            with st.spinner(f\"Generating answer using {selected_model}...\"):\n",
        "                result = st.session_state.rag_system.query(query)\n",
        "\n",
        "            # Display answer\n",
        "            st.subheader(\"Answer\")\n",
        "            st.write(result[\"answer\"])\n",
        "\n",
        "            # Display retrieved documents\n",
        "            st.subheader(\"Sources\")\n",
        "            for i, doc in enumerate(result[\"retrieved_docs\"]):\n",
        "                with st.expander(f\"Source {i+1} ({doc['type']}, Page {doc['page']})\"):\n",
        "                    st.write(doc[\"content\"])\n",
        "        except ValueError as ve:\n",
        "            st.error(f\"RAG system error: {ve}\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"An unexpected error occurred: {e}\")\n",
        "\n",
        "# Add model information in the sidebar\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.subheader(\"Model Information\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "- **T5 Small (Hugging Face)**: Lightweight model, fastest but less capable (use with caution)\n",
        "- **Phi-3.5-mini-instruct (LM Studio)**: Microsoft's compact but powerful model (via LM Studio)\n",
        "- **Falcon-7B (LM Studio)**: Larger model with good performance (via LM Studio)\n",
        "- **Llama 3.1 (LM Studio)**: Meta's latest model, most capable but requires more resources (via LM Studio)\n",
        "\"\"\")\n",
        "\n",
        "# Footer\n",
        "st.markdown(\"---\")\n",
        "st.markdown(\"Multimodal RAG System Demo - WAI Team 3\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jwnT-zxxQjas"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MUILTIMODEL RAG**"
      ],
      "metadata": {
        "id": "wHZ2uzk2REBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import pytesseract\n",
        "from pdf2image import convert_from_path\n",
        "from PyPDF2 import PdfReader\n",
        "\n",
        "import streamlit as st\n",
        "import traceback\n",
        "\n",
        "# LangChain imports\n",
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
        "from langchain.chains import create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# Loaders\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# Embeddings and models\n",
        "from langchain_community.embeddings import OpenAIEmbeddings\n",
        "from langchain_community.llms import OpenAI\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "\n",
        "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
        "\n",
        "class MultimodalRAGWithModelSelection:\n",
        "    def __init__(self, model_name=\"google/flan-t5-small\"):\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        st.write(f\"Using device: {self.device}\")\n",
        "\n",
        "        self.pdf_path = None\n",
        "        self.combined_vectors = None\n",
        "        self.llm = None\n",
        "        self.retriever = None\n",
        "        self.rag_chain = None\n",
        "        self.current_model = model_name\n",
        "\n",
        "        with st.spinner(\"Loading embedding model...\"):\n",
        "            self.text_embedder = HuggingFaceEmbeddings(\n",
        "                model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "            )\n",
        "\n",
        "        with st.spinner(f\"Loading language model: {model_name}...\"):\n",
        "            self._initialize_llm(model_name)\n",
        "\n",
        "    def _initialize_llm(self, model_name):\n",
        "        self.current_model = model_name\n",
        "\n",
        "        try:\n",
        "            if any(keyword in model_name.lower() for keyword in [\"llama\", \"phi\", \"falcon\"]):\n",
        "                self.llm = ChatOpenAI(\n",
        "                    model_name=model_name,\n",
        "                    openai_api_key=\"DUMMY_KEY\",\n",
        "                    openai_api_base=\"http://localhost:1234/v1\",\n",
        "                    temperature=0.7,\n",
        "                    max_tokens=512\n",
        "                )\n",
        "                st.success(f\"Connected to LM Studio with model: {model_name}\")\n",
        "            elif model_name == \"google/flan-t5-small\":\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                gen_pipeline = pipeline(\n",
        "                    \"text2text-generation\",\n",
        "                    model=model_name,\n",
        "                    tokenizer=tokenizer,\n",
        "                    max_length=512,\n",
        "                    device=0 if self.device == \"cuda\" else -1\n",
        "                )\n",
        "                self.llm = HuggingFacePipeline(pipeline=gen_pipeline)\n",
        "                st.success(f\"Loaded Hugging Face model: {model_name}\")\n",
        "            else:\n",
        "                self.llm = None\n",
        "                st.error(f\"Model {model_name} not supported.\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error initializing model: {e}\")\n",
        "            self.llm = None\n",
        "\n",
        "        if self.retriever is not None:\n",
        "            self._build_rag_chain()\n",
        "\n",
        "    def change_model(self, new_model_name):\n",
        "        if new_model_name != self.current_model:\n",
        "            with st.spinner(f\"Loading new model: {new_model_name}...\"):\n",
        "                self._initialize_llm(new_model_name)\n",
        "\n",
        "\n",
        "    def process_pdf(self, pdf_path: str):\n",
        "        self.pdf_path = pdf_path\n",
        "        st.write(\"Processing PDF...\")\n",
        "\n",
        "        with st.spinner(\"Extracting text...\"):\n",
        "            text_docs = self._extract_text()\n",
        "            st.write(f\"Extracted {len(text_docs)} text chunks\")\n",
        "\n",
        "        with st.spinner(\"Extracting images and performing OCR...\"):\n",
        "            image_docs = self._extract_images()\n",
        "            st.write(f\"Extracted {len(image_docs)} image documents\")\n",
        "\n",
        "        with st.spinner(\"Creating vector database...\"):\n",
        "            self._create_vector_stores(text_docs, image_docs)\n",
        "\n",
        "        with st.spinner(\"Building RAG chain...\"):\n",
        "            self._build_rag_chain()\n",
        "\n",
        "        st.success(\"PDF processing complete!\")\n",
        "\n",
        "        return {\n",
        "            \"text_docs\": len(text_docs),\n",
        "            \"image_docs\": len(image_docs)\n",
        "        }\n",
        "\n",
        "    def _extract_text(self) -> List[Document]:\n",
        "        try:\n",
        "            loader = PyPDFLoader(self.pdf_path)\n",
        "            documents = loader.load()\n",
        "\n",
        "            text_splitter = RecursiveCharacterTextSplitter(\n",
        "                chunk_size=1000,\n",
        "                chunk_overlap=200,\n",
        "                separators=[\"\\n\\n\", \"\\n\", \".\", \"!\", \"?\", \",\", \" \", \"\"],\n",
        "                keep_separator=True\n",
        "            )\n",
        "\n",
        "            for i, doc in enumerate(documents):\n",
        "                doc.metadata[\"source\"] = self.pdf_path\n",
        "                doc.metadata[\"page\"] = i + 1\n",
        "                doc.metadata[\"type\"] = \"text\"\n",
        "\n",
        "                headings = re.findall(r'^(#+)\\s+(.+)$', doc.page_content, re.MULTILINE)\n",
        "                if headings:\n",
        "                    doc.metadata[\"headings\"] = [h[1] for h in headings]\n",
        "\n",
        "            return text_splitter.split_documents(documents)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error in _extract_text: {e}\")\n",
        "            st.error(traceback.format_exc())\n",
        "            return []\n",
        "\n",
        "\n",
        "    def _extract_images(self) -> List[Document]:\n",
        "        images = convert_from_path(self.pdf_path)\n",
        "        image_docs = []\n",
        "\n",
        "        for i, img in enumerate(images):\n",
        "            ocr_text = pytesseract.image_to_string(img)\n",
        "            if len(ocr_text.strip()) > 20:\n",
        "                doc = Document(\n",
        "                    page_content=ocr_text,\n",
        "                    metadata={\n",
        "                        \"source\": self.pdf_path,\n",
        "                        \"page\": i + 1,\n",
        "                        \"type\": \"image\"\n",
        "                    }\n",
        "                )\n",
        "                image_docs.append(doc)\n",
        "\n",
        "        return image_docs\n",
        "\n",
        "    def _create_vector_stores(self, text_docs, image_docs):\n",
        "        self.text_vectors = FAISS.from_documents(text_docs, self.text_embedder)\n",
        "\n",
        "        if image_docs:\n",
        "            self.image_vectors = FAISS.from_documents(image_docs, self.text_embedder)\n",
        "            self.combined_vectors = self.text_vectors\n",
        "            self.combined_vectors.merge_from(self.image_vectors)\n",
        "        else:\n",
        "            self.combined_vectors = self.text_vectors\n",
        "\n",
        "    def _build_rag_chain(self):\n",
        "        prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        Answer the question based ONLY on the provided context.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {input}\n",
        "\n",
        "        Instructions:\n",
        "        1. Only use information from the provided context\n",
        "        2. If the context contains tables or charts, analyze them\n",
        "        3. If you don't know the answer based on the context, say \"I don't have enough information to answer this question.\"\n",
        "        4. Provide a detailed answer\n",
        "\n",
        "        Answer:\n",
        "        \"\"\")\n",
        "\n",
        "        self.retriever = self.combined_vectors.as_retriever(\n",
        "            search_type=\"similarity\",\n",
        "            search_kwargs={\"k\": 5}\n",
        "        )\n",
        "\n",
        "        document_chain = create_stuff_documents_chain(self.llm, prompt)\n",
        "        self.rag_chain = create_retrieval_chain(self.retriever, document_chain)\n",
        "\n",
        "    def query(self, question: str) -> Dict[str, Any]:\n",
        "        if not self.rag_chain:\n",
        "            raise ValueError(\"RAG chain not initialized. Process a PDF first.\")\n",
        "        if self.llm is None:\n",
        "            raise ValueError(\"Language model is not initialized. Please select a valid model.\")\n",
        "\n",
        "        response = self.rag_chain.invoke({\"input\": question})\n",
        "\n",
        "        retrieved_docs = []\n",
        "        for doc in response.get(\"context\", []):\n",
        "            doc_info = {\n",
        "                \"content\": doc.page_content[:200] + \"...\" if len(doc.page_content) > 200 else doc.page_content,\n",
        "                \"type\": doc.metadata.get(\"type\", \"unknown\"),\n",
        "                \"page\": doc.metadata.get(\"page\", \"unknown\")\n",
        "            }\n",
        "            retrieved_docs.append(doc_info)\n",
        "\n",
        "        return {\n",
        "            \"answer\": response.get(\"answer\", \"No answer generated\"),\n",
        "            \"retrieved_docs\": retrieved_docs\n",
        "        }\n"
      ],
      "metadata": {
        "id": "sfzcLiLVQtix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **RUN**"
      ],
      "metadata": {
        "id": "gqayBsF9RWTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "from dotenv import load_dotenv\n",
        "import asyncio\n",
        "import sys\n",
        "\n",
        "import sys\n",
        "\n",
        "# Prevent torch introspection bug in Windows + Streamlit\n",
        "if \"torch._classes\" in sys.modules:\n",
        "    del sys.modules[\"torch._classes\"]\n",
        "\n",
        "if sys.platform == \"win32\":\n",
        "    asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())\n",
        "\n",
        "import sys\n",
        "sys.path.append('./app')\n",
        "\n",
        "# Get the absolute path to the project root directory (where run.py is)\n",
        "project_root = os.path.abspath(os.path.dirname(__file__))\n",
        "sys.path.insert(0, project_root)\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "# Import the main app\n",
        "from app.main import *\n",
        "\n",
        "# streamlit run run.py\n",
        "\n",
        "#streamlit run app/main.py"
      ],
      "metadata": {
        "id": "_YmEhoUMQ5L5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}